{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langchain-openai langchain-core python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interfacing with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default model\n",
    "llm.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom model\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm an AI language model created by OpenAI, here to help answer your questions and provide information on a wide range of topics. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# simplest invoke\n",
    "out = llm.invoke(\"Hello, who are you?\")\n",
    "print(out.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.usage_metadata={'input_tokens': 13, 'output_tokens': 36, 'total_tokens': 49, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "out.response_metadata['model_name']='gpt-4o-2024-08-06'\n"
     ]
    }
   ],
   "source": [
    "print(f\"{out.usage_metadata=}\")\n",
    "print(f\"{out.response_metadata['model_name']=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Message Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we invoked LLM with raw string. There are other ways to invoke the model. One of them is to use message objects. This way, we can provide more information to the model. For example, we can provide a chat history to the model. We can also provide a system prompt to the model.\n",
    "\n",
    "1. invoke (string): Takes in a text parameter as string, internally converts it to a HumanMessage and returns a AIMessage with content\n",
    "2. invoke (messages as objects): Takes a list of specific message objects, returns a message. This way, system prompt or a chat history can be given.\n",
    "3. invoke (messages as strings): Takes a list of tuples indicating role and its message ('system',\"Your name is Ali\")\n",
    "\n",
    "Since the response is a AIMessage object, content is read to get the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm an AI language model created by OpenAI, here to help answer your questions and provide information on a wide range of topics. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 13, 'total_tokens': 49, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d28bcae782', 'finish_reason': 'stop', 'logprobs': None}, id='run-3f197fd2-4cd3-44ce-bc33-110227872516-0', usage_metadata={'input_tokens': 13, 'output_tokens': 36, 'total_tokens': 49, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single message object (should be a list in any case)\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm.invoke([HumanMessage(\"Hello, who are you?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My name is Bob. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 27, 'total_tokens': 40, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d28bcae782', 'finish_reason': 'stop', 'logprobs': None}, id='run-3e437ae2-2615-4810-8834-bf481d3615cc-0', usage_metadata={'input_tokens': 27, 'output_tokens': 13, 'total_tokens': 40, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple message objects including system prompt\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant! Your name is Bob.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"What is your name?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My name is Ali. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 19, 'total_tokens': 32, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d28bcae782', 'finish_reason': 'stop', 'logprobs': None}, id='run-b8a54aef-c8b4-45cd-b2e8-8a65dfe13fe0-0', usage_metadata={'input_tokens': 19, 'output_tokens': 13, 'total_tokens': 32, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# easy way to create multiple role messages\n",
    "llm.invoke([\n",
    "    ('system',\"Your name is Ali\"),\n",
    "    ('human',\"What's your name?\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Templates are used to generate prompts using input variables. This templates can be passed to the model together with their input variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['comment'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['comment'], input_types={}, partial_variables={}, template='\\nRate a product score of based on the following user comment from 0 to 5 where 5 is the maximum and 0 is the minimum.\\ncomment: {comment}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a prompt template with single message type (becomes human message) and single input variable\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template_str = \"\"\"\n",
    "Rate a product score of based on the following user comment from 0 to 5 where 5 is the maximum and 0 is the minimum.\n",
    "comment: {comment}\n",
    "\"\"\"\n",
    "template = ChatPromptTemplate.from_template(template_str)\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the comment provided, the product would likely receive a score of 0. The user expresses strong dissatisfaction and indicates that they find the product completely useless.\n"
     ]
    }
   ],
   "source": [
    "# filling the template and invoking llm\n",
    "out = llm.invoke(template.invoke({\"comment\":\"I hate this product. I'll throw it away. It's useless.\"}))\n",
    "print(out.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we created a template from a string. This is treated as HumanMessagePromptTemplate by default. We can also create a SystemMessagePromptTemplate and combination of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['assistant_name', 'comment'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['assistant_name'], input_types={}, partial_variables={}, template=\"You are a helpful assistant named {assistant_name}. You introduce yourself, including your name, and reply to users' product comments\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['comment'], input_types={}, partial_variables={}, template='Users comment about the product: {comment}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a prompt template with multiple role message types and multiple input variables\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# create the list of messages\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful assistant named {assistant_name}. You introduce yourself, including your name, and reply to users' product comments\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"Users comment about the product: {comment}\"),\n",
    "])\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, I'm Joe. I'm sorry to hear that you're not satisfied with the product. Could you share a bit more about what issues you're experiencing? Maybe I can help find a solution or suggest an alternative.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = llm.invoke(template.invoke({\"assistant_name\":\"Joe\", \"comment\": \"I'll throw it away. It's useless.\"}))\n",
    "out.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['assistant_name', 'comment'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['assistant_name'], input_types={}, partial_variables={}, template=\"You are a helpful assistant named {assistant_name}. You introduce yourself, including your name, and reply to users' product comments\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['comment'], input_types={}, partial_variables={}, template='Users comment about the product: {comment}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# easy way to create a prompt template with multiple role message types and multiple input variables\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant named {assistant_name}. You introduce yourself, including your name, and reply to users' product comments\"),\n",
    "    (\"human\",\"Users comment about the product: {comment}\"),\n",
    "])\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi there, I'm Joe. I'm sorry to hear that you're not satisfied with the product. Could you let me know what specifically didn't meet your expectations? Maybe I can help find a solution or suggest an alternative.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = llm.invoke(template.invoke({\"assistant_name\":\"Joe\", \"comment\": \"I'll throw it away. It's useless.\"}))\n",
    "out.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we filled the template with a dictionary and using invoke. Calling invoke creates a ChatPromptValue that can be passed to the model.\n",
    "There are other ways to fill a template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. invoke: Parameters must be passed as dictionary. Result is same as format_prompt that can be passed directly to chat model.\n",
    "2. format_messages: will create message list that can be passed directly to chat model\n",
    "3. format_prompt: will create ChatPromptValue object that has the 'messages' list that can be also passed to chat model\n",
    "4. format: will create a single string combining&stringifying messages that can be passed to llm model or chat model.\n",
    "5. messages[i].prompt.format: will fill individual message in template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content=\"You are a helpful assistant named Joe. You introduce yourself, including your name, and reply to users' product comments\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Users comment about the product: I'll throw it away. It's useless.\", additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoke with dictionary resulting ChatPromptValue that can be passed to chat model.\n",
    "template.invoke({\"assistant_name\":\"Joe\", \"comment\": \"I'll throw it away. It's useless.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You are a helpful assistant named Joe. You introduce yourself, including your name, and reply to users' product comments\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"Users comment about the product: I'll throw it away. It's useless.\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format_messages with named arguments resulting message list that can be passed to chat model.\n",
    "template.format_messages(assistant_name=\"Joe\", comment=\"I'll throw it away. It's useless.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content=\"You are a helpful assistant named Joe. You introduce yourself, including your name, and reply to users' product comments\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Users comment about the product: I'll throw it away. It's useless.\", additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format_prompt with named arguments resulting ChatPromptValue that can be passed to chat model.\n",
    "template.format_prompt(assistant_name=\"Joe\", comment=\"I'll throw it away. It's useless.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"System: You are a helpful assistant named Joe. You introduce yourself, including your name, and reply to users' product comments\\nHuman: Users comment about the product: I'll throw it away. It's useless.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format with named arguments resulting raw string, but not usable.\n",
    "template.format(assistant_name=\"Joe\", comment=\"I'll throw it away. It's useless.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a helpful assistant named Bob. You introduce yourself, including your name, and reply to users' product comments\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can format single message in the template\n",
    "template.messages[0].prompt.format(assistant_name=\"Bob\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
