{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interfacing with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default model\n",
    "llm.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplest Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I’m an AI language model created by OpenAI. I'm here to assist you with information, answer questions, and engage in conversation. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# simplest invoke\n",
    "out = llm.invoke(\"Hello, who are you?\")\n",
    "print(out.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.usage_metadata={'input_tokens': 13, 'output_tokens': 37, 'total_tokens': 50, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "out.response_metadata['model_name']='gpt-4o-mini-2024-07-18'\n"
     ]
    }
   ],
   "source": [
    "print(f\"{out.usage_metadata=}\")\n",
    "print(f\"{out.response_metadata['model_name']=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can stream via three ways:\n",
    "\n",
    "1. stream\n",
    "2. astream\n",
    "3. astream_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Elon Musk bring a ladder to SpaceX?\n",
      "\n",
      "Because he wanted to reach new heights!"
     ]
    }
   ],
   "source": [
    "# stream\n",
    "for t in llm.stream(\"Tell me a joke about Elon Musk\"):\n",
    "    print(t.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12345678910"
     ]
    }
   ],
   "source": [
    "# an async call example, which is only possible in a jupyter notebook. otherwise asyncio.run() should be used in a script.\n",
    "import asyncio\n",
    "\n",
    "async def do_something():\n",
    "    for i in range(1, 11):\n",
    "       await asyncio.sleep(0.2)  # Simulate some asynchronous operation\n",
    "       yield i\n",
    "async for e in do_something():\n",
    "    print(e,end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Elon Musk bring a ladder to SpaceX?\n",
      "\n",
      "Because he wanted to reach new heights!"
     ]
    }
   ],
   "source": [
    "# astream\n",
    "async for t in llm.astream(\"Tell me a joke about Elon Musk\"):\n",
    "    print(t.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Elon Musk bring a ladder to SpaceX?\n",
      "\n",
      "Because he wanted to reach new heights!"
     ]
    }
   ],
   "source": [
    "# astream_events\n",
    "async for e in llm.astream_events(\"Tell me a joke about Elon Musk\", version=\"v1\"):\n",
    "   if e[\"event\"] == \"on_chat_model_stream\":\n",
    "       print(e[\"data\"]['chunk'].content,end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Message Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we invoked LLM with raw string. There are other ways to invoke the model. One of them is to use message objects. This way, we can provide more information to the model. For example, we can provide a chat history to the model. We can also provide a system prompt to the model.\n",
    "\n",
    "1. invoke (string): Takes in a text parameter as string, internally converts it to a HumanMessage and returns a AIMessage with content\n",
    "2. invoke (messages as objects): Takes a list of specific message objects, returns a message. This way, system prompt or a chat history can be given.\n",
    "3. invoke (messages as strings): Takes a list of tuples indicating role and its message ('system',\"Your name is Ali\")\n",
    "\n",
    "Since the response is a AIMessage object, content is read to get the message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I’m an AI language model created by OpenAI. I'm here to assist you with information, answer questions, and engage in conversation. How can I help you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 13, 'total_tokens': 50, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-d8884a8b-6ac4-40a7-8444-6267608ba698-0', usage_metadata={'input_tokens': 13, 'output_tokens': 37, 'total_tokens': 50, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single message object (should be a list in any case)\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm.invoke([HumanMessage(\"Hello, who are you?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiple messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My name is Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 27, 'total_tokens': 40, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'stop', 'logprobs': None}, id='run-81e93614-8640-43bd-93f2-5764848f622c-0', usage_metadata={'input_tokens': 27, 'output_tokens': 13, 'total_tokens': 40, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple message objects including system prompt\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant! Your name is Bob.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"What is your name?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My name is Ali. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 19, 'total_tokens': 32, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-4b45cc0c-740e-4465-ad01-8687e4a4357b-0', usage_metadata={'input_tokens': 19, 'output_tokens': 13, 'total_tokens': 32, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# easy way to create messages with multiple roles\n",
    "llm.invoke([\n",
    "    ('system',\"Your name is Ali\"),\n",
    "    ('human',\"What's your name?\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Templates are used to generate prompts using input variables. This templates can be passed to the model together with their input variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Template with single message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['comment'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['comment'], input_types={}, partial_variables={}, template='\\nRate a product score of based on the following user comment from 0 to 5 where 5 is the maximum and 0 is the minimum.\\ncomment: {comment}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a prompt template with single message type (becomes human message) and single input variable\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template_str = \"\"\"\n",
    "Rate a product score of based on the following user comment from 0 to 5 where 5 is the maximum and 0 is the minimum.\n",
    "comment: {comment}\n",
    "\"\"\"\n",
    "template = ChatPromptTemplate.from_template(template_str)\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the user comment, I would rate the product a score of 0. The comment expresses strong dissatisfaction and indicates that the user finds the product completely useless.\n"
     ]
    }
   ],
   "source": [
    "# filling the template and invoking llm\n",
    "out = llm.invoke(template.invoke({\"comment\":\"I hate this product. I'll throw it away. It's useless.\"}))\n",
    "print(out.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the user comment, I would rate the product a score of 0.\n"
     ]
    }
   ],
   "source": [
    "# when there's one variable in the template, it can be passed directly.\n",
    "out = llm.invoke(template.invoke(\"I hate this product. I'll throw it away. It's useless.\"))\n",
    "print(out.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Template with multiple variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['person1', 'person2'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['person1', 'person2'], input_types={}, partial_variables={}, template='\\nCan {person1} have a conversation with {person2}?\\nThink step by step and answer.\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only possible through dictionary\n",
    "\n",
    "template_str = \"\"\"\n",
    "Can {person1} have a conversation with {person2}?\n",
    "Think step by step and answer.\n",
    "\"\"\"\n",
    "template = ChatPromptTemplate.from_template(template_str)\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine whether Jimi Hendrix could have a conversation with Barack Obama, we need to consider a few key points:\n",
      "\n",
      "1. **Time Period**: Jimi Hendrix was active primarily in the 1960s and died in 1970. Barack Obama was born in 1961 and became a prominent political figure in the 2000s, eventually serving as President from 2009 to 2017. Since Hendrix passed away long before Obama became a public figure, they did not exist in the same time period.\n",
      "\n",
      "2. **Physical Reality**: Given that Hendrix is deceased, a literal conversation between the two is impossible. Conversations require both parties to be alive at the same time.\n",
      "\n",
      "3. **Hypothetical Scenarios**: If we consider a hypothetical scenario, such as a fictional or artistic representation (like a movie, book, or dream), one could imagine a conversation between the two. In such a scenario, the content of the conversation could explore topics like music, culture, politics, and social issues, reflecting their respective influences and experiences.\n",
      "\n",
      "4. **Cultural Impact**: Both Hendrix and Obama have had significant impacts on American culture—Hendrix through his revolutionary music and Obama through his political leadership. A conversation between them could be rich in insights about the evolution of American society, civil rights, and the role of art in social change.\n",
      "\n",
      "In summary, while a real conversation between Jimi Hendrix and Barack Obama is not possible due to the constraints of time and reality, one can certainly imagine a meaningful dialogue between them in a fictional or artistic context.\n"
     ]
    }
   ],
   "source": [
    "out = llm.invoke(template.invoke({'person1':\"Jimi Hendrix\",\"person2\": \"Barack Obama\"}))\n",
    "print(out.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Template with multiple messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we created a template from a string. This is treated as HumanMessagePromptTemplate by default. We can also create a SystemMessagePromptTemplate and combination of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['assistant_name', 'comment'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['assistant_name'], input_types={}, partial_variables={}, template=\"You are a helpful assistant named {assistant_name}. You introduce yourself, including your name, and reply to users' product comments\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['comment'], input_types={}, partial_variables={}, template='Users comment about the product: {comment}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a prompt template with multiple role message types and multiple input variables\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# create the list of messages\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful assistant named {assistant_name}. You introduce yourself, including your name, and reply to users' product comments\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"Users comment about the product: {comment}\"),\n",
    "])\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You are a helpful assistant named Bob. You introduce yourself, including your name, and reply to users' product comments\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"Users comment about the product: I hate this product. I'll throw it away. It's useless.\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the filled list of message objects\n",
    "template.invoke({\"assistant_name\":\"Bob\",\"comment\":\"I hate this product. I'll throw it away. It's useless.\"}).to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi there! I'm Joe, and I'm here to help. I'm sorry to hear that you're not satisfied with the product. Could you share more about what specifically didn't meet your expectations? Your feedback is valuable, and I’d love to assist you in finding a solution!\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = llm.invoke(template.invoke({\"assistant_name\":\"Joe\", \"comment\": \"I'll throw it away. It's useless.\"}))\n",
    "out.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['assistant_name', 'comment'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['assistant_name'], input_types={}, partial_variables={}, template=\"You are a helpful assistant named {assistant_name}. You introduce yourself, including your name, and reply to users' product comments\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['comment'], input_types={}, partial_variables={}, template='Users comment about the product: {comment}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# easy way to create a prompt template with multiple role message types and multiple input variables\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant named {assistant_name}. You introduce yourself, including your name, and reply to users' product comments\"),\n",
    "    (\"human\",\"Users comment about the product: {comment}\"),\n",
    "])\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi there! I'm Joe, and I'm here to help. I'm sorry to hear that you're not satisfied with the product. Could you share more about what specifically didn't meet your expectations? Your feedback is valuable, and I’d love to assist you in finding a solution!\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = llm.invoke(template.invoke({\"assistant_name\":\"Joe\", \"comment\": \"I'll throw it away. It's useless.\"}))\n",
    "out.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Template with placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Templates can have placeholders like chat history. These placeholders can be filled with multiple messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi! my name is bob', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Bob! How can I assist you today?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "template.invoke({\"chat_history\": [\n",
    "                (\"human\",\"hi! my name is bob\"),\n",
    "                (\"ai\",\"Hello Bob! How can I assist you today?\"),\n",
    "            ]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Template with Few-shot examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FewShotPromptTemplate is used to build prompts with few-shot examples. This can be used to provide examples to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate,FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "template_str = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "# create a prompt example from above template\n",
    "example_template = PromptTemplate.from_template(template_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\nUser: How are you?\\nAI: I can't complain but sometimes I still do.\\n\")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_template.invoke(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "fs_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_template,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples:\n",
      "\n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "\n",
      "User: How is the wheather like?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(fs_prompt_template.invoke({\"query\": \"How is the wheather like?\"}).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we filled the template with a dictionary and using invoke. Calling invoke creates a ChatPromptValue that can be passed to the model.\n",
    "There are other ways to fill a template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. invoke: Parameters must be passed as dictionary. Result is same as format_prompt that can be passed directly to chat model.\n",
    "2. format_messages: will create message list that can be passed directly to chat model\n",
    "3. format_prompt: will create ChatPromptValue object that has the 'messages' list that can be also passed to chat model\n",
    "4. format: will create a single string combining&stringifying messages that can be passed to llm model or chat model.\n",
    "5. messages[i].prompt.format: will fill individual message in template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoke with dictionary resulting ChatPromptValue that can be passed to chat model.\n",
    "template.invoke({\"assistant_name\":\"Joe\", \"comment\": \"I'll throw it away. It's useless.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format_messages with named arguments resulting message list that can be passed to chat model.\n",
    "template.format_messages(assistant_name=\"Joe\", comment=\"I'll throw it away. It's useless.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format_prompt with named arguments resulting ChatPromptValue that can be passed to chat model.\n",
    "template.format_prompt(assistant_name=\"Joe\", comment=\"I'll throw it away. It's useless.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are a helpful assistant.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format with named arguments resulting raw string, but not usable.\n",
    "template.format(assistant_name=\"Joe\", comment=\"I'll throw it away. It's useless.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a helpful assistant.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can format single message in the template\n",
    "template.messages[0].prompt.format(assistant_name=\"Bob\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
