{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Basics notebook, we created templates and invoked them and finally feed the result to the LLM. We can make this easier by using Langchain expression language (LCEL) which is very similar to the piping in the Linux shells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Iâ€™m an AI language model created by OpenAI. I'm here to assist you with information, answer questions, and engage in conversation. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "out = llm.invoke(\"Hello, who are you?\")\n",
    "print(out.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple invokes without chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the user comment, I would rate the product a score of 5. The enthusiasm and positive language indicate a very high level of satisfaction.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 52, 'total_tokens': 83, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-0c2248ce-569c-4572-a5c4-69c59a8a3f20-0', usage_metadata={'input_tokens': 52, 'output_tokens': 31, 'total_tokens': 83, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Rate a product score of based on the following user comment from 0 to 5 where 5 is the maximum and 0 is the minimum.\n",
    "comment: {comment}\n",
    "\"\"\")\n",
    "llm.invoke(template.invoke({\"comment\": \"I love this product, it is the best thing ever!\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single invoke with chaining  \n",
    "\n",
    "In the chaining, we use the `|` operator. When the input is provided to the chain, template being the first layer in chain, will be invoked to create the input to the next layer by filling the template. Next layer in the chain, LLM, will be invoked with the output of the previous layer and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "templated_llm = template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the user comment, I would rate the product a score of 5. The enthusiasm and positive language indicate a very high level of satisfaction.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 52, 'total_tokens': 83, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f2cd28694a', 'finish_reason': 'stop', 'logprobs': None}, id='run-ef1efb70-2892-4347-ac92-10d6184b2441-0', usage_metadata={'input_tokens': 52, 'output_tokens': 31, 'total_tokens': 83, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templated_llm.invoke({\"comment\": \"I love this product, it is the best thing ever!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the user comment, I would rate the product a score of 5. The enthusiasm and positive language indicate a very high level of satisfaction.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 52, 'total_tokens': 83, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'stop', 'logprobs': None}, id='run-68190604-3242-4e46-99a7-29af7f5605fd-0', usage_metadata={'input_tokens': 52, 'output_tokens': 31, 'total_tokens': 83, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# again if there's only one variable, you can just pass it directly\n",
    "templated_llm.invoke(\"I love this product, it is the best thing ever!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of langchain chains is the runnable abstraction. Many classes implement the runnable interface that allows them to be invoked in the chain. \n",
    "A custom runnable function can be created using RunnableLamdba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO WORLD'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A runnable that returns the length of input string when invoked\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "str_uppercaser = RunnableLambda(lambda s: s.upper())\n",
    "str_uppercaser.invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A runnable that returns reversed input string when invoked\n",
    "str_reverser = RunnableLambda(lambda s: s[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DLROW OLLEH'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A chained runnable that returns reversed and uppercased input string when invoked\n",
    "(str_reverser | str_uppercaser).invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'length': 11, 'input': 'dlrow olleH'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A lambda can be directly passed to the chain as well, it will be converted to a RunnableLambda implicitly.\n",
    "chain = str_reverser | \\\n",
    "        (lambda s: {\"length\": len(s), \"input\": s})\n",
    "chain.invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RunnablePassthrough is a runnable that simply passes the input to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dlrow olleH'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# this way it's not useful but to demonstrate.\n",
    "(str_reverser | RunnablePassthrough()).invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'foo': 10, 'bar': 17}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it can be used to extend input&output dictionaries.\n",
    "runnable1 = RunnableLambda(lambda x: x[\"foo\"] + 7)\n",
    "chain = RunnablePassthrough.assign(bar=runnable1)\n",
    "chain.invoke({\"foo\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A chained runnable that returns the filled template and llm response to this filled template.\n",
    "# It also demonstrates how to create a Runnable(Parallel) from a dictionaru\n",
    "templated_llm = template | {\n",
    "    'question': RunnablePassthrough(), # template output is passed through\n",
    "    'llm_response': llm # Parsing logic\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the user comment \"Tell me a joke about a chicken,\" it seems the user is looking for humor or entertainment rather than providing feedback on a product. Therefore, I would rate the product score as 2. This indicates a low level of satisfaction or relevance to the product, as the comment does not provide any specific feedback or critique.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 48, 'total_tokens': 118, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-c401a788-aa73-40f0-b021-04138e950212-0', usage_metadata={'input_tokens': 48, 'output_tokens': 70, 'total_tokens': 118, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templated_llm.invoke(\"Tell me a joke about a chicken\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RunnableParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs the runnables in parallel and combines the results in the given parameters. Either a dictionary or named arguments can be passed. Function names or lambdas can be passed, they are converted to RunnableLambda implicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'len': 11, 'reverse': 'dlrow olleH'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passing dictionary directly\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "RunnableParallel({\n",
    "    \"len\": lambda s: len(s),\n",
    "    \"reverse\": lambda s: s[::-1]\n",
    "}).invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'len': 11, 'reverse': 'dlrow olleH'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passing named args\n",
    "RunnableParallel(\n",
    "    len=lambda s: len(s),\n",
    "    reverse=lambda s: s[::-1]\n",
    ").invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it really execute in parallel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_running_func(s):\n",
    "    import time\n",
    "    for i in range(10):\n",
    "        time.sleep(1)\n",
    "        print(f\"Processing '{s}', step{i+1}\")\n",
    "    return f\"Processed {s}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def another_long_running_func(s):\n",
    "    import time\n",
    "    for i in range(10):\n",
    "        time.sleep(1)\n",
    "        print(f\"Optimizing '{s}', step{i+1}\")\n",
    "    return f\"Optimized {s}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'Hello world', step1\n",
      "Optimizing 'Hello world', step1\n",
      "Processing 'Hello world', step2\n",
      "Optimizing 'Hello world', step2\n",
      "Processing 'Hello world', step3\n",
      "Optimizing 'Hello world', step3\n",
      "Processing 'Hello world', step4\n",
      "Optimizing 'Hello world', step4\n",
      "Optimizing 'Hello world', step5\n",
      "Processing 'Hello world', step5\n",
      "Optimizing 'Hello world', step6\n",
      "Processing 'Hello world', step6\n",
      "Optimizing 'Hello world', step7\n",
      "Processing 'Hello world', step7\n",
      "Processing 'Hello world', step8Optimizing 'Hello world', step8\n",
      "\n",
      "Optimizing 'Hello world', step9\n",
      "Processing 'Hello world', step9\n",
      "Processing 'Hello world', step10\n",
      "Optimizing 'Hello world', step10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'func1': 'Processed Hello world', 'func2': 'Optimized Hello world'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnableParallel(\n",
    "    func1=long_running_func,\n",
    "    func2=another_long_running_func,\n",
    ").invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @chain decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to build chains is @chain decorator. It allows a more customable way to build chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reimplement templated_llm using chain decorator\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig, chain\n",
    "\n",
    "@chain\n",
    "def templated_llm(user_input: str, config:RunnableConfig):\n",
    "    value = template.invoke({\"comment\": user_input}, config=config)\n",
    "    return llm.invoke(value, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the user comment, I would rate the product a score of 5. The comment expresses strong positive feelings and satisfaction with the product.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 52, 'total_tokens': 82, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'stop', 'logprobs': None}, id='run-598db142-a115-4924-bf6b-aa3927d5364c-0', usage_metadata={'input_tokens': 52, 'output_tokens': 30, 'total_tokens': 82, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templated_llm.invoke(\"I love this product, it is the best thing ever!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binding is used to add default invocation parameters to a runnable. For example, invoke of ChatOpenAI has a stop parameter to stop generating tokens. This can be set with bind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_template(\n",
    "    \"Tell me 3 intersting facts about {subject}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "templated_llm = template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are three interesting facts about cats:\n",
      "\n",
      "1. **Unique Communication**: Cats have a unique way of communicating with humans that differs from how they interact with other cats. While they use meowing primarily to communicate with humans, adult cats typically do not meow at each other. Instead, they use body language, purring, and other vocalizations to communicate with their feline peers.\n",
      "\n",
      "2. **Whisker Sensitivity**: A cat's whiskers are highly sensitive tactile hairs called vibrissae. They are deeply embedded in the cat's skin and are packed with nerve endings, allowing cats to detect changes in their environment, navigate in the dark, and gauge whether they can fit through tight spaces. Whiskers can also help cats sense nearby objects and even changes in air currents.\n",
      "\n",
      "3. **Sleep Patterns**: Cats are known for their love of sleep, and they can sleep anywhere from 12 to 16 hours a day, with some cats sleeping even more. This behavior is a result of their evolutionary history as predators; they conserve energy by sleeping for long periods and are most active during dawn and dusk, a behavior known as crepuscular activity.\n"
     ]
    }
   ],
   "source": [
    "# output without stopping\n",
    "out = templated_llm.invoke({\"subject\": \"cats\"})\n",
    "print(out.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are three interesting facts about cats:\n"
     ]
    }
   ],
   "source": [
    "# binding stop parameter to llm\n",
    "templated_llm = template | llm.bind(stop=\"\\n\")\n",
    "out = templated_llm.invoke({\"subject\": \"cats\"})\n",
    "print(out.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we get AIMessage object as the output. We can parse the output to get the desired information. The simplest method is the get the string directly from the AIMessage object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "templated_llm = prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the chicken join a band?\\n\\nBecause it had the drumsticks!'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templated_llm.invoke(\"chicken\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
