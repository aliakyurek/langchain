{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Basics notebook, we created templates and invoked them and finally feed the result to the LLM. We can make this easier by using Langchain expression language (LCEL) which is very similar to the piping in the Linux shells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm an AI language model created by OpenAI. I'm here to assist you with information, answer questions, and engage in conversation. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "out = llm.invoke(\"Hello, who are you?\")\n",
    "print(out.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple invokes without chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the user comment, I would rate the product a score of 5. The enthusiasm and positive language indicate a very high level of satisfaction.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 52, 'total_tokens': 83, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-7bccba71-1f80-41df-8897-b44555871e47-0', usage_metadata={'input_tokens': 52, 'output_tokens': 31, 'total_tokens': 83, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Rate a product score of based on the following user comment from 0 to 5 where 5 is the maximum and 0 is the minimum.\n",
    "comment: {comment}\n",
    "\"\"\")\n",
    "llm.invoke(template.invoke({\"comment\": \"I love this product, it is the best thing ever!\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single invoke with chaining  \n",
    "\n",
    "In the chaining, we use the `|` operator. When the input is provided to the chain, template being the first layer in chain, will be invoked to create the input to the next layer by filling the template. Next layer in the chain, LLM, will be invoked with the output of the previous layer and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "templated_llm = template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the positive sentiment expressed in the comment, I would rate the product a score of 5.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 52, 'total_tokens': 74, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-0b3cedc9-164b-444d-9851-4963c01c63e7-0', usage_metadata={'input_tokens': 52, 'output_tokens': 22, 'total_tokens': 74, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templated_llm.invoke({\"comment\": \"I love this product, it is the best thing ever!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the user comment, I would rate the product a score of 5. The enthusiasm and positive language indicate a very high level of satisfaction.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 52, 'total_tokens': 83, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'stop', 'logprobs': None}, id='run-6122831d-125f-4007-af1f-f851e874af7b-0', usage_metadata={'input_tokens': 52, 'output_tokens': 31, 'total_tokens': 83, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# again if there's only one variable, you can just pass it directly\n",
    "templated_llm.invoke(\"I love this product, it is the best thing ever!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of langchain chains is the runnable abstraction. Many classes implement the runnable interface that allows them to be invoked in the chain. \n",
    "A custom runnable function can be created using RunnableLamdba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO WORLD'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A runnable that returns the length of input string when invoked\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "str_uppercaser = RunnableLambda(lambda s: s.upper())\n",
    "str_uppercaser.invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A runnable that returns reversed input string when invoked\n",
    "str_reverser = RunnableLambda(lambda s: s[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DLROW OLLEH'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A chained runnable that returns reversed and uppercased input string when invoked\n",
    "(str_reverser | str_uppercaser).invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'length': 11, 'input': 'dlrow olleH'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A lambda can be directly passed to the chain as well, it will be converted to a RunnableLambda implicitly.\n",
    "chain = str_reverser | \\\n",
    "        (lambda s: {\"length\": len(s), \"input\": s})\n",
    "chain.invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RunnablePassthrough is a runnable that simply passes the input to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dlrow olleH'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# this way it's not useful but to demonstrate.\n",
    "(str_reverser | RunnablePassthrough()).invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'foo': 10, 'bar': 17}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it can be used to extend input&output dictionaries.\n",
    "runnable1 = RunnableLambda(lambda x: x[\"foo\"] + 7)\n",
    "chain = RunnablePassthrough.assign(bar=runnable1)\n",
    "chain.invoke({\"foo\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A chained runnable that returns the filled template and llm response to this filled template.\n",
    "# It also demonstrates how to create a Runnable(Parallel) from a dictionaru\n",
    "templated_llm = template | {\n",
    "    'question': RunnablePassthrough(), # template output is passed through\n",
    "    'llm_response': llm # Parsing logic\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': ChatPromptValue(messages=[HumanMessage(content='\\nRate a product score of based on the following user comment from 0 to 5 where 5 is the maximum and 0 is the minimum.\\ncomment: Tell me a joke about a chicken\\n', additional_kwargs={}, response_metadata={})]),\n",
       " 'llm_response': AIMessage(content='Based on the user comment \"Tell me a joke about a chicken,\" I would rate the product score as a 2. The comment suggests a light-hearted request, but it doesn\\'t provide any specific feedback or indicate satisfaction with a product. It seems more like a playful interaction rather than a review of a product\\'s quality or performance.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 48, 'total_tokens': 115, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-c79d4714-47a8-4892-9261-269f29c0446e-0', usage_metadata={'input_tokens': 48, 'output_tokens': 67, 'total_tokens': 115, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templated_llm.invoke(\"Tell me a joke about a chicken\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RunnableParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs the runnables in parallel and combines the results in the given parameters. Either a dictionary or named arguments can be passed. Function names or lambdas can be passed, they are converted to RunnableLambda implicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'len': 11, 'reverse': 'dlrow olleH'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passing dictionary directly\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "RunnableParallel({\n",
    "    \"len\": lambda s: len(s),\n",
    "    \"reverse\": lambda s: s[::-1]\n",
    "}).invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'len': 11, 'reverse': 'dlrow olleH'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passing named args\n",
    "RunnableParallel(\n",
    "    len=lambda s: len(s),\n",
    "    reverse=lambda s: s[::-1]\n",
    ").invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it really execute in parallel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_running_func(s):\n",
    "    import time\n",
    "    for i in range(10):\n",
    "        time.sleep(1)\n",
    "        print(f\"Processing '{s}', step{i+1}\")\n",
    "    return f\"Processed {s}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def another_long_running_func(s):\n",
    "    import time\n",
    "    for i in range(10):\n",
    "        time.sleep(1)\n",
    "        print(f\"Optimizing '{s}', step{i+1}\")\n",
    "    return f\"Optimized {s}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'Hello world', step1\n",
      "Optimizing 'Hello world', step1\n",
      "Processing 'Hello world', step2Optimizing 'Hello world', step2\n",
      "\n",
      "Processing 'Hello world', step3Optimizing 'Hello world', step3\n",
      "\n",
      "Optimizing 'Hello world', step4\n",
      "Processing 'Hello world', step4\n",
      "Optimizing 'Hello world', step5Processing 'Hello world', step5\n",
      "\n",
      "Processing 'Hello world', step6Optimizing 'Hello world', step6\n",
      "\n",
      "Processing 'Hello world', step7Optimizing 'Hello world', step7\n",
      "\n",
      "Processing 'Hello world', step8Optimizing 'Hello world', step8\n",
      "\n",
      "Optimizing 'Hello world', step9Processing 'Hello world', step9\n",
      "\n",
      "Optimizing 'Hello world', step10\n",
      "Processing 'Hello world', step10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'func1': 'Processed Hello world', 'func2': 'Optimized Hello world'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnableParallel(\n",
    "    func1=long_running_func,\n",
    "    func2=another_long_running_func,\n",
    ").invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @chain decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to build chains is @chain decorator. It allows a more customable way to build chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reimplement templated_llm using chain decorator\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig, chain\n",
    "\n",
    "@chain\n",
    "def templated_llm(user_input: str, config:RunnableConfig):\n",
    "    value = template.invoke({\"comment\": user_input}, config=config)\n",
    "    return llm.invoke(value, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the user comment, I would rate the product a score of 5. The enthusiasm and positive language indicate a very high level of satisfaction.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 52, 'total_tokens': 83, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-2fc3059a-55ce-4455-9e0b-8e2c5ee37ca5-0', usage_metadata={'input_tokens': 52, 'output_tokens': 31, 'total_tokens': 83, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templated_llm.invoke(\"I love this product, it is the best thing ever!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binding is used to add default invocation parameters to a runnable. For example, invoke of ChatOpenAI has a stop parameter to stop generating tokens. This can be set with bind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_template(\n",
    "    \"Tell me 3 intersting facts about {subject}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "templated_llm = template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are three interesting facts about cats:\n",
      "\n",
      "1. **Unique Communication**: Cats have a unique way of communicating with humans that differs from how they interact with other cats. While they use meowing primarily to communicate with humans, they rarely meow at each other as adults. Instead, they use body language, purring, and other vocalizations to communicate with fellow felines.\n",
      "\n",
      "2. **Whisker Sensitivity**: A cat's whiskers are not just for show; they are highly sensitive tactile hairs called vibrissae. These whiskers can detect changes in the environment, helping cats navigate in the dark and judge the width of openings. Whiskers are so sensitive that they can even pick up on subtle changes in air currents.\n",
      "\n",
      "3. **Sleep Patterns**: Cats are known for their love of sleep, and they can sleep anywhere from 12 to 16 hours a day, with some cats sleeping even more. This behavior is rooted in their evolutionary history as predators; sleeping allows them to conserve energy for hunting. Interestingly, cats are crepuscular, meaning they are most active during dawn and dusk, aligning their hunting instincts with the behavior of their natural prey.\n"
     ]
    }
   ],
   "source": [
    "# output without stopping\n",
    "out = templated_llm.invoke({\"subject\": \"cats\"})\n",
    "print(out.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are three interesting facts about cats:\n"
     ]
    }
   ],
   "source": [
    "# binding stop parameter to llm\n",
    "templated_llm = template | llm.bind(stop=\"\\n\")\n",
    "out = templated_llm.invoke({\"subject\": \"cats\"})\n",
    "print(out.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we get AIMessage object as the output. We can parse the output to get the desired information. The simplest method is the get the string directly from the AIMessage object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "templated_llm = prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the chicken join a band?\\n\\nBecause it had the drumsticks!'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templated_llm.invoke(\"chicken\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
